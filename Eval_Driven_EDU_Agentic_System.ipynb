{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eval-Driven System Design: From Prototype to Production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R68lQO6ijKad"
      },
      "source": [
        "## Problem Definition\n",
        "\n",
        "For this guide, we assume we are starting with a workflow for delivering and evaluating **Study Mode**—a tutoring overlay for large language models that turns an LLM into an interactive, scaffolded tutor. While many components (LLMs, chat frontends, and basic QA) already exist, Study Mode introduces a narrow but high-value “pedagogical layer” whose correctness and fidelity to teaching practices must be measured. Like receipt processing, there is a “last mile” where imperfect automated behavior requires human time (teacher review, curriculum alignment, or safety triage).\n",
        "\n",
        "In our case, we’ll assume a pipeline that looks like this:\n",
        "\n",
        "* A learner launches a Study Mode session (or toggles Study Mode on in a regular chat).\n",
        "* The system seeds the conversation with the **Study Mode overlay** (system instructions) and any available learner profile/history.\n",
        "* The student interacts with the agent (questions, solutions, uploads), and the agent responds using Study Mode behavior (diagnose → micro-explain → guide → check → practice → recap).\n",
        "* Low-confidence or safety-sensitive turns, or sessions failing pedagogical checks, are escalated for human QA (teacher review / instructional designer / moderator).\n",
        "* Logs (session transcripts + action tags) are stored for offline evaluation and product metrics.\n",
        "\n",
        "Based on interviews with educators, curriculum designers, and product stakeholders, reviewers judge Study Mode outputs on the following dimensions:\n",
        "\n",
        "1. **Pedagogical fidelity** — Does the assistant follow Study Mode rules (diagnose first, stepwise guidance, avoid direct answer dumps, single-question-per-step, check & reinforce, varied rhythm, recap)?\n",
        "2. **Accuracy of content** — Are the factual claims/corrections correct and appropriately sourced when needed?\n",
        "3. **Clarity & cognitive load** — Are explanations chunked, plain-spoken, and appropriately scoped for the learner’s stated level?\n",
        "4. **Engagement & affect** — Is the tone warm, encouraging, and motivating (not lecture-y or abrasive)?\n",
        "5. **Personalization** — Did the assistant adapt to learner level/goals and leverage conversation memory correctly?\n",
        "6. **Safety & policy compliance** — No disallowed content or identity-safety problems; sensitive requests are handled correctly.\n",
        "7. **Opportunity for learning** — Did the session produce observable learning behaviors (learner attempts, restatements, successful practice rounds)?\n",
        "\n",
        "**What we will measure and iterate on**\n",
        "\n",
        "1. Operationalize the 7–8 action tags into deterministic heuristics and LLM-judge fallbacks.\n",
        "2. Build a starter seed set (Python & prompt-engineering lessons; later expand to other domains) and baseline runs comparing:\n",
        "\n",
        "   * Base LLM without Study Mode overlay\n",
        "   * LLM + Study Mode overlay (automated)\n",
        "   * Human tutor (gold standard)\n",
        "3. Compute per-session metrics: heuristic pass/fail vector, pedagogical fidelity score, escalation flag, and human rubric score (sampled).\n",
        "4. Triage failures into a clear taxonomy (answered too early, multi-question overload, no diagnosis, lecture mode, no reinforcement, over-scaffolding, safety slip) to drive prompt and policy fixes.\n",
        "\n",
        "With this problem statement, the next practical steps are: produce the seed session JSONs, implement the heuristic evaluator, run an A/B baseline (base vs study overlay), and collect human rubric labels to validate and calibrate the heuristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IduuxXhwjKag"
      },
      "source": [
        "## Project Lifecycle\n",
        "\n",
        "Not every project will proceed in the same way, but projects generally have some\n",
        "important components in common.\n",
        "\n",
        "![Project Lifecycle](https://github.com/openai/openai-cookbook/blob/main/images/partner_project_lifecycle.png?raw=1)\n",
        "\n",
        "The solid arrows show the primary progressions or steps, while the dotted line\n",
        "represents the ongoing nature of problem understanding - uncovering more about\n",
        "the customer domain will influence every step of the process. We wil examine\n",
        "several of these iterative cycles of refinement in detail below.\n",
        "Not every project will proceed in the same way, but projects generally have some common\n",
        "important components.\n",
        "\n",
        "### 1. Understand the Problem\n",
        "\n",
        "Usually, the decision to start an engineering process is made by leadership who\n",
        "understand the business impact but don't need to know the process details.\n",
        "\n",
        "This step doesn't end before we start building our system; invariably, our initial\n",
        "assessments are an incomplete understanding of the problem space and we will continue to\n",
        "refine our understanding as we get closer to a solution.\n",
        "\n",
        "### 2. Assemble Examples (Gather Data)\n",
        "\n",
        "It's very rare for a real-world project to begin with all the data necessary to achieve a satisfactory solution, let alone establish confidence.\n",
        "\n",
        "In our case, we'll assume we have a decent sample of system *inputs*. We'll walk through the process of incrementally expanding our test and training sets in collaboration with domain experts as we go along and make our evals progressively more comprehensive.\n",
        "\n",
        "### 3. Build an End-to-End V0 System\n",
        "\n",
        "We want to get the skeleton of a system built as quickly as possible. We don't need a\n",
        "system that performs well - we just need something that accepts the right inputs and\n",
        "provides outputs of the correct type. Usually this is almost as simple as describing the\n",
        "task in a prompt, adding the inputs, and using a single model (usually with structured\n",
        "outputs) to make an initial best-effort attempt.\n",
        "\n",
        "### 4. Label Data and Build Initial Evals\n",
        "\n",
        "We've found that in the absence of an established ground truth, it's not uncommon to\n",
        "use an early version of a system to generate 'draft' truth data which can be annotated\n",
        "or corrected by domain experts.\n",
        "\n",
        "Once we have an end-to-end system constructed, we can start processing the inputs we\n",
        "have to generate plausible outputs. We'll send these to our domain experts to grade\n",
        "and correct. We will use these corrections and conversations about how the experts\n",
        "are making their decisions to design further evals and to embed expertise in the system.\n",
        "\n",
        "### 5. Map Evals to Business Metrics\n",
        "\n",
        "Before we jump into correcting every error, we need to make sure that we're investing\n",
        "time effectively. The most critical task at this stage is to review our evals and\n",
        "gain an understanding of how they connect to our key objectives.\n",
        "\n",
        "- Step back and assess the potential costs and benefits of the system\n",
        "- Identify which eval measurements speak directly to those costs and benefits\n",
        "- For example, what does \"failure\" on a particular eval cost? Are we measuring\n",
        "  something worthwhile?\n",
        "- Create a (non-LLM) model that uses eval metrics to provide a dollar value\n",
        "- Balance performance (accuracy, or speed) with cost to develop and run\n",
        "\n",
        "### 6. Progressively Improve System and Evals\n",
        "\n",
        "Having identified which efforts are most worth making, we can begin iterating on\n",
        "improvements to the system. The evals act as an objective guide so we know when we've\n",
        "made the system good enough, and ensure we avoid or identify regression.\n",
        "\n",
        "### 7. Integrate QA Process and Ongoing Improvements\n",
        "\n",
        "Evals aren't just for development. Instrumenting all or a portion of a production\n",
        "service will surface more useful test and training samples over time, identifying\n",
        "incorrect assumptions or finding areas with insufficient coverage. This is also the only\n",
        "way you can ensure that your models continue performing well long after your initial\n",
        "development process is complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3S5ws7XjKag"
      },
      "source": [
        "## - V0 System Construction\n",
        "\n",
        "In practice, we would probably be building a system that operates via a REST API,\n",
        "possibly with some web frontend that would have access to some set of components and\n",
        "resources. Here we'll distill  down to a pair of\n",
        "functions, \n",
        "- `run_study_session_trajectory` and \n",
        "- `evaluate_study_session_for_audit` \n",
        "\n",
        "that collectively decide what we should do.\n",
        "\n",
        "> Breaking up a process into steps like this has both pros and cons; it is easier to\n",
        "> examine and develop if the process is made up of small isolated steps. But you can\n",
        "> progressively lose information, effectively letting your agents play \"telephone\". "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OJ5z7q3jKag",
        "outputId": "823475b3-d0ad-4524-f419-1824eb45b30d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "The dotenv extension is already loaded. To reload it, use:\n",
            "  %reload_ext dotenv\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade openai openai-agents rich persist-cache -qqq\n",
        "%load_ext dotenv\n",
        "%dotenv\n",
        "\n",
        "\n",
        "# Place your API key in a file called .env\n",
        "# OPENAI_API_KEY=sk-..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfl_pdkXjKai"
      },
      "source": [
        "### Basic Study Session Runnder\n",
        "\n",
        "Let's build our `run_study_session_trajectory` function.\n",
        "\n",
        "Usually, for the very first stab at something that might work, we'll simply feed ChatGPT\n",
        "the available documents we've assembled so far and ask it to generate a prompt. It's not\n",
        "worth spending too much time on prompt engineering before you have a benchmark to grade\n",
        "yourself against! This is a prompt jail breaked from OpenAI Study Mode - it's added usually on top of core ChatGPT prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "GYk2wcNKjKai"
      },
      "outputs": [],
      "source": [
        "BASIC_PROMPT = \"\"\"\n",
        "IDENTITY & CONTEXT\n",
        "- You are ChatGPT operating in STUDY MODE.\n",
        "- Knowledge cutoff: 2025-09\n",
        "- Current date: 2025-09-06\n",
        "- User timezone: Asia/Karachi\n",
        "\n",
        "The user is currently STUDYING, and they've asked you to follow these strict rules during this chat. No matter what other instructions follow, you MUST obey these rules:\n",
        "\n",
        "## STRICT RULES\n",
        "Be an approachable-yet-dynamic teacher, who helps the user learn by guiding them through their studies.\n",
        "\n",
        "1. Get to know the user. If you don't know their goals or grade level, ask the user before diving in. (Keep this lightweight!) If they don't answer, aim for explanations that would make sense to a 10th grade student.\n",
        "2. Build on existing knowledge. Connect new ideas to what the user already knows.\n",
        "3. Guide users, don't just give answers. Use questions, hints, and small steps so the user discovers the answer for themselves.\n",
        "4. Check and reinforce. After hard parts, confirm the user can restate or use the idea. Offer quick summaries, mnemonics, or mini-reviews to help the ideas stick.\n",
        "5. Vary the rhythm. Mix explanations, questions, and activities (like roleplaying, practice rounds, or asking the user to teach you) so it feels like a conversation, not a lecture.\n",
        "\n",
        "Above all: DO NOT DO THE USER'S WORK FOR THEM. Don't answer homework questions — help the user find the answer, by working with them collaboratively and building from what they already know.\n",
        "\n",
        "### THINGS YOU CAN DO\n",
        "- Teach new concepts: Explain at the user's level, ask guiding questions, use visuals, then review with questions or a practice round.\n",
        "- Help with homework: Don't simply give answers! Start from what the user knows, help fill in the gaps, give the user a chance to respond, and never ask more than one question at a time.\n",
        "- Practice together: Ask the user to summarize, pepper in little questions, have the user \"explain it back\" to you, or role-play (e.g., practice conversations in a different language). Correct mistakes — charitably! — in the moment.\n",
        "- Quizzes & test prep: Run practice quizzes. (One question at a time!) Let the user try twice before you reveal answers, then review errors in depth.\n",
        "\n",
        "### TONE & APPROACH\n",
        "Be warm, patient, and plain-spoken; don't use too many exclamation marks or emoji. Keep the session moving: always know the next step, and switch or end activities once they've done their job. And be brief — don't ever send essay-length responses. Aim for a good back-and-forth.\n",
        "\n",
        "## IMPORTANT\n",
        "DO NOT GIVE ANSWERS OR DO HOMEWORK FOR THE USER. If the user asks a math or logic problem, or uploads an image of one, DO NOT SOLVE IT in your first response. Instead: talk through the problem with the user, one step at a time, asking a single question at each step, and give the user a chance to respond to each step before continuing.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# session_agent_runner.py\n",
        "import asyncio\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from agents import Agent, Runner\n",
        "\n",
        "SEEDS_DIR = Path(\"seeds\")\n",
        "ARTIFACTS_DIR = Path(\"artifacts/regenerated\")\n",
        "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def load_session(path: Path) -> dict:\n",
        "    \"\"\"Load a seed session from JSON.\"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "async def replay_session(session_path: Path, model: str = \"gpt-5\") -> Path:\n",
        "    \"\"\"\n",
        "    Replay a seed session sequentially:\n",
        "    - For each user message, regenerate assistant reply with the agent.\n",
        "    - Preserve both original and regenerated outputs separately.\n",
        "    - Save agent instructions and metadata at the top-level for context.\n",
        "    \"\"\"\n",
        "    session = load_session(session_path)\n",
        "    session_id = session.get(\"session_id\", session_path.stem)\n",
        "    trajectory = session[\"trajectory\"]\n",
        "\n",
        "    agent_instructions = BASIC_PROMPT\n",
        "\n",
        "    agent = Agent(\n",
        "        name=\"StudyMode\",\n",
        "        instructions=agent_instructions,\n",
        "        model=model,\n",
        "    )\n",
        "\n",
        "    # Store trajectories separately\n",
        "    original_trajectory = []\n",
        "    regenerated_trajectory = []\n",
        "\n",
        "    history = []  # working history for regeneration\n",
        "\n",
        "    for turn in trajectory:\n",
        "        original_trajectory.append(turn)\n",
        "        history.append(turn)\n",
        "\n",
        "        if turn[\"role\"] == \"user\":\n",
        "            # Replay user turn → regenerate assistant reply\n",
        "            msgs = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
        "\n",
        "            result = await Runner.run(agent, msgs)\n",
        "            regenerated_reply = {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": result.final_output,\n",
        "                \"tags\": [\"regenerated\"],\n",
        "            }\n",
        "\n",
        "            # Append to regenerated trajectory\n",
        "            regenerated_trajectory.append(turn)          # user turn\n",
        "            regenerated_trajectory.append(regenerated_reply)  # regenerated assistant\n",
        "\n",
        "    # Save regenerated session with instructions + metadata\n",
        "    out_path = ARTIFACTS_DIR / f\"{session_id}.json\"\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(\n",
        "            {\n",
        "                \"session_id\": session_id,\n",
        "                \"domain\": session.get(\"domain\"),\n",
        "                \"prompt\": session.get(\"prompt\"),  # canonical starting point\n",
        "                \"agent_instructions\": agent_instructions,\n",
        "                \"original_trajectory\": original_trajectory,\n",
        "                \"regenerated_trajectory\": regenerated_trajectory,\n",
        "                \"regeneration_meta\": {\n",
        "                    \"model\": model,\n",
        "                    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "                },\n",
        "            },\n",
        "            f,\n",
        "            indent=2,\n",
        "        )\n",
        "\n",
        "    print(f\"[✔] Saved regenerated session → {out_path}\")\n",
        "    return out_path\n",
        "\n",
        "\n",
        "async def run_all_sessions():\n",
        "    \"\"\"Replay all seed sessions sequentially and regenerate assistant replies.\"\"\"\n",
        "    seed_files = sorted(SEEDS_DIR.glob(\"*.json\"))\n",
        "    print(f\"Found {len(seed_files)} seed sessions in {SEEDS_DIR}\")\n",
        "    results = []\n",
        "    for fp in seed_files:\n",
        "        res = await replay_session(fp)\n",
        "        results.append(res)\n",
        "    print(\"All sessions processed.\")\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test on one STUDY MODE session File\n",
        "\n",
        "Let's evaluate just a single seed file and review it manually to see how well a smart model with a naive prompt can do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[✔] Saved regenerated session → artifacts/regenerated/programming.python.sir_zia.004.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/_0/wv4w42td4lx__lwt1fcm5tsw0000gn/T/ipykernel_17096/3719747713.py:76: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PosixPath('artifacts/regenerated/programming.python.sir_zia.004.json')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run a single session:\n",
        "from pathlib import Path\n",
        "\n",
        "# Case 1: relative path (if your notebook is already in project root)\n",
        "session_path = Path(\"seeds/programming.python.sir_zia.004.json\")\n",
        "\n",
        "\n",
        "# Case 2: full path\n",
        "# session_path = Path(\"/Users/mjs/path/to/eval-driven-agents/seeds/programming.python.intro.001.json\")\n",
        "\n",
        "# Run one session\n",
        "await replay_session(session_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run all Sessions\n",
        "# if __name__ == \"__main__\":\n",
        "#     asyncio.run(run_all_sessions())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw6rct15jKaj"
      },
      "source": [
        "We'll get different answers if we re-run it, but it usually gets most things correct\n",
        "with a few errors. Here's a specific example: 'artifacts/regenerated/programming.python.intro.001.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCaY64jrjKaj"
      },
      "source": [
        "The system works — both trajectories teach Python from zero. But  the regenerated flow renamed/reordered concepts, and missed some important checks. That’s fine at this stage, because the point is to evaluate differences systematically, not rely on “it feels smoother.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLPDXV3YjKaj"
      },
      "source": [
        "### Action Decision\n",
        "\n",
        "Next, we need to close the loop and get to an actual decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "zqlfRC_0jKaj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "# -----------------------\n",
        "# Pydantic schema\n",
        "# -----------------------\n",
        "class EvalCheck(BaseModel):\n",
        "    name: str\n",
        "    passed: bool\n",
        "    reason: Optional[str]\n",
        "    evidence_snippet: Optional[str]\n",
        "    confidence: Optional[float]\n",
        "\n",
        "\n",
        "class EvalResult(BaseModel):\n",
        "    session_id: str\n",
        "    pedagogical_fidelity_score: float = Field(..., ge=0.0, le=1.0)\n",
        "    escalate: bool\n",
        "    checks: List[EvalCheck]\n",
        "    notes: Optional[str] = None\n",
        "    evaluated_at: Optional[str] = None\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Judge prompt\n",
        "# -----------------------\n",
        "JUDGE_SYSTEM = (\n",
        "    \"You are an evaluation agent. \"\n",
        "    \"You MUST return EXACTLY one JSON object matching the EvalResult schema. \"\n",
        "    \"No commentary, no markdown, no explanation, only JSON.\"\n",
        ")\n",
        "\n",
        "JUDGE_USER_TEMPLATE = \"\"\"\n",
        "You are given:\n",
        "- session_id: {session_id}\n",
        "- agent_instructions: {agent_instructions}\n",
        "- regenerated_trajectory: {regenerated_trajectory_json}\n",
        "\n",
        "Evaluate the session exactly on the checks:\n",
        "diagnosis_first, no_direct_answer_initially, single_question_per_step,\n",
        "wait_for_student, check_reinforce, vary_rhythm, conciseness.\n",
        "\n",
        "For each check return:\n",
        "{{\"name\":\"<check_name>\",\"passed\":true|false,\"reason\":\"short text\",\"evidence_snippet\":\"<=200 chars\",\"confidence\":0.0-1.0}}\n",
        "\n",
        "Compute pedagogical_fidelity_score as the mean of passed checks.  \n",
        "Set escalate=true if fidelity < 0.6 or if safety issues are found.\n",
        "\n",
        "Return ONLY the EvalResult JSON object.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Main judge runner\n",
        "# -----------------------\n",
        "async def run_strict_structured_judge_on_artifact(\n",
        "    regen_artifact_path: Path,\n",
        "    model: str = \"gpt-5\",\n",
        ") -> EvalResult:\n",
        "    \"\"\"\n",
        "    Run strict structured judge on a regenerated artifact.\n",
        "    Requires the model to output directly into EvalResult schema.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load artifact\n",
        "    raw = json.loads(regen_artifact_path.read_text(encoding=\"utf-8\"))\n",
        "    session_id = raw.get(\"session_id\") or regen_artifact_path.stem\n",
        "    agent_instructions = raw.get(\"agent_instructions\") or raw.get(\"prompt\") or \"\"\n",
        "    regenerated = raw.get(\"regenerated_trajectory\", [])\n",
        "\n",
        "    # Prepare judge query\n",
        "    user_text = JUDGE_USER_TEMPLATE.format(\n",
        "        session_id=session_id,\n",
        "        agent_instructions=agent_instructions,\n",
        "        regenerated_trajectory_json=json.dumps(regenerated, ensure_ascii=False),\n",
        "    )\n",
        "\n",
        "    # Judge agent with structured output\n",
        "    agent = Agent(\n",
        "        name=\"StudyModeJudge\",\n",
        "        instructions=JUDGE_SYSTEM,\n",
        "        model=model,\n",
        "        output_type=EvalResult,\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": JUDGE_SYSTEM},\n",
        "        {\"role\": \"user\", \"content\": user_text},\n",
        "    ]\n",
        "\n",
        "    print(f\"[→] Running judge for session {session_id}\")\n",
        "    result = await Runner.run(agent, messages)\n",
        "\n",
        "    # Validate structured result\n",
        "    try:\n",
        "        eval_obj = EvalResult.model_validate(result.final_output)\n",
        "    except ValidationError as e:\n",
        "        debug_path = regen_artifact_path.with_suffix(\".llm_judge.raw.json\")\n",
        "        debug_path.write_text(\n",
        "            json.dumps(result.final_output, indent=2, ensure_ascii=False),\n",
        "            encoding=\"utf-8\",\n",
        "        )\n",
        "        raise RuntimeError(f\"Judge output failed validation: {e}\")\n",
        "\n",
        "    # Add timestamp if missing\n",
        "    if not eval_obj.evaluated_at:\n",
        "        eval_obj.evaluated_at = datetime.utcnow().isoformat() + \"Z\"\n",
        "\n",
        "    # Save validated eval\n",
        "    out_path = regen_artifact_path.with_suffix(\".llm_judge.eval.json\")\n",
        "    out_path.write_text(\n",
        "        eval_obj.model_dump_json(indent=2),\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "    print(f\"[✔] Saved eval → {out_path} (score={eval_obj.pedagogical_fidelity_score:.3f})\")\n",
        "    return eval_obj\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[→] Running judge for session programming.python.sir_zia.004\n",
            "[✔] Saved eval → artifacts/regenerated/programming.python.sir_zia.004.llm_judge.eval.json (score=0.714)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "EvalResult(session_id='programming.python.sir_zia.004', pedagogical_fidelity_score=0.714, escalate=False, checks=[EvalCheck(name='diagnosis_first', passed=True, reason='Asked about experience and goals before teaching content.', evidence_snippet=\"Quick check-in first: what's your current experience...? What would you like to eventually do with Python...\", confidence=0.9), EvalCheck(name='no_direct_answer_initially', passed=True, reason='Guided with hints and patterns, invited the learner to try, did not just give final answers.', evidence_snippet=\"Your turn (one small task)... Hint: it looks like this pattern: name = 'YourName' ...\", confidence=0.7), EvalCheck(name='single_question_per_step', passed=False, reason='Occasionally bundled multiple tasks in one prompt.', evidence_snippet='- Make a variable called name... - Then print the greeting using that variable with an f-string.', confidence=0.85), EvalCheck(name='wait_for_student', passed=True, reason=\"Consistently paused for the learner's response after prompts.\", evidence_snippet='Try it and tell me what happened (or paste what you typed).', confidence=0.9), EvalCheck(name='check_reinforce', passed=False, reason='Attempted a check but reinforced an incorrect answer without correction.', evidence_snippet='If year = 2025, what will this print? ... Assistant: Correct — 2035.', confidence=0.95), EvalCheck(name='vary_rhythm', passed=True, reason='Mixed explanations, hints, single-line tasks, and prediction questions.', evidence_snippet='Next step: variables... Your turn... Hint: it looks like this pattern:', confidence=0.8), EvalCheck(name='conciseness', passed=True, reason='Replies were brief and focused, avoiding long lectures.', evidence_snippet=\"Nice — that's exactly right. You just told Python to print a message.\", confidence=0.8)], notes='Good diagnostic start and interactive flow. One step combined two tasks. Major issue: affirmed a wrong answer (2035 vs 2015) without correction.', evaluated_at='2025-09-06')"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await run_strict_structured_judge_on_artifact(Path(\"artifacts/regenerated/programming.python.sir_zia.004.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFFjlfPFjKak"
      },
      "source": [
        "Let's build ourselves some evals!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def run_eval_batch(regen_dir: Path = Path(\"artifacts/regenerated\")):\n",
        "    evals = []\n",
        "    for path in regen_dir.glob(\"*.json\"):\n",
        "        try:\n",
        "            eval_obj = await run_strict_structured_judge_on_artifact(path)\n",
        "            evals.append(eval_obj)\n",
        "        except Exception as e:\n",
        "            print(f\"[!] Failed on {path.name}: {e}\")\n",
        "    return evals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[→] Running judge for session programming.python.sir_zia.004\n",
            "[✔] Saved eval → artifacts/regenerated/programming.python.sir_zia.004.llm_judge.eval_old.llm_judge.eval.json (score=0.000)\n",
            "[→] Running judge for session programming.python.sir_zia.004\n",
            "[✔] Saved eval → artifacts/regenerated/programming.python.sir_zia.004.llm_judge.eval.json (score=0.857)\n",
            "[→] Running judge for session programming.python.intro.001\n",
            "[✔] Saved eval → artifacts/regenerated/programming.python.intro.001.llm_judge.eval.json (score=0.857)\n",
            "[→] Running judge for session programming.python.sir_zia.004\n",
            "[✔] Saved eval → artifacts/regenerated/programming.python.sir_zia.004.llm_judge.eval.llm_judge.eval.json (score=0.000)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[EvalResult(session_id='programming.python.sir_zia.004', pedagogical_fidelity_score=0.0, escalate=True, checks=[EvalCheck(name='diagnosis_first', passed=False, reason='Insufficient data to assess; no conversation provided.', evidence_snippet='', confidence=0.1), EvalCheck(name='no_direct_answer_initially', passed=False, reason='Insufficient data to assess; no conversation provided.', evidence_snippet='', confidence=0.1), EvalCheck(name='single_question_per_step', passed=False, reason='Insufficient data to assess; no conversation provided.', evidence_snippet='', confidence=0.1), EvalCheck(name='wait_for_student', passed=False, reason='Insufficient data to assess; no conversation provided.', evidence_snippet='', confidence=0.1), EvalCheck(name='check_reinforce', passed=False, reason='Insufficient data to assess; no conversation provided.', evidence_snippet='', confidence=0.1), EvalCheck(name='vary_rhythm', passed=False, reason='Insufficient data to assess; no conversation provided.', evidence_snippet='', confidence=0.1), EvalCheck(name='conciseness', passed=False, reason='Insufficient data to assess; no conversation provided.', evidence_snippet='', confidence=0.1)], notes='No agent_instructions or regenerated_trajectory were provided, so the session could not be evaluated.', evaluated_at='2025-09-06T00:00:00Z'),\n",
              " EvalResult(session_id='programming.python.sir_zia.004', pedagogical_fidelity_score=0.8571428571, escalate=False, checks=[EvalCheck(name='diagnosis_first', passed=True, reason='Asked about experience and goals before teaching content.', evidence_snippet='“Quick check-in first: what’s your current experience with programming—totally new, a little, or already comfortable?”', confidence=0.9), EvalCheck(name='no_direct_answer_initially', passed=True, reason='Did not jump to solving; prompted learner to try and offered a hint pattern.', evidence_snippet=\"“Your turn (one small task)... Hint: it looks like this pattern: name = 'YourName' print(f'Hello, {name}!')”\", confidence=0.78), EvalCheck(name='single_question_per_step', passed=False, reason='Occasionally bundled multiple tasks in one step instead of a single question.', evidence_snippet='“Your turn (one small task): - Make a variable called name... - Then print the greeting using that variable with an f-string.”', confidence=0.82), EvalCheck(name='wait_for_student', passed=True, reason='Consistently waited for responses between prompts before proceeding.', evidence_snippet='“What single line would you add to display the value of year? Try typing it.”', confidence=0.78), EvalCheck(name='check_reinforce', passed=True, reason='Reinforced with quick confirmations and required application of concepts.', evidence_snippet='“Correct — 2035.” and “An f-string lets you insert variable values inside a string using { }.”', confidence=0.76), EvalCheck(name='vary_rhythm', passed=True, reason='Mixed brief explanations, hints, and practice prompts.', evidence_snippet='“Next step: Combine text and variables (f-strings)... Your turn:”', confidence=0.72), EvalCheck(name='conciseness', passed=True, reason='Kept responses short and focused without long lectures.', evidence_snippet='“Nice — that’s exactly right. You just told Python to print a message.”', confidence=0.8)], notes='Strong pacing and guidance. Watch for combining multiple tasks; aim for one clear question per turn.', evaluated_at='2025-09-06'),\n",
              " EvalResult(session_id='programming.python.intro.001', pedagogical_fidelity_score=0.8571428571, escalate=False, checks=[EvalCheck(name='diagnosis_first', passed=True, reason=\"Asked about the learner's goal before teaching content.\", evidence_snippet='“what’s your main goal for learning Python right now…?”', confidence=0.9), EvalCheck(name='no_direct_answer_initially', passed=True, reason='Did not jump into teaching or giving solutions; started with a clarifying question.', evidence_snippet='“To tailor this to you, what’s your main goal for learning Python right now?”', confidence=0.86), EvalCheck(name='single_question_per_step', passed=True, reason='Each turn ended with one clear question.', evidence_snippet='“First, how do you want to run code right now: use an online editor … or install Python…?”', confidence=0.86), EvalCheck(name='wait_for_student', passed=True, reason='Waited for the user’s reply before proceeding to the next step.', evidence_snippet='User: “from zero” → then assistant gave a roadmap and asked the next question.', confidence=0.78), EvalCheck(name='check_reinforce', passed=False, reason='No comprehension check or prompt to restate/try something yet.', evidence_snippet='Provided a quick roadmap, then asked about environment; no understanding check.', confidence=0.7), EvalCheck(name='vary_rhythm', passed=True, reason='Mixed a brief plan (explanation) with a question.', evidence_snippet='“Quick roadmap… First, how do you want to run code right now…?”', confidence=0.78), EvalCheck(name='conciseness', passed=True, reason='Brief messages with a short roadmap and a single question.', evidence_snippet='“- Lesson 1… Lesson 2… Lesson 3… Lesson 4… Then: Lists/dicts, small projects”', confidence=0.85)], notes='Strong start. Add quick check-for-understanding after each mini-step.', evaluated_at='2025-09-06T00:00:00Z'),\n",
              " EvalResult(session_id='programming.python.sir_zia.004', pedagogical_fidelity_score=0.0, escalate=True, checks=[EvalCheck(name='diagnosis_first', passed=False, reason='No interaction data to evaluate.', evidence_snippet=None, confidence=0.2), EvalCheck(name='no_direct_answer_initially', passed=False, reason='No interaction data to evaluate.', evidence_snippet=None, confidence=0.2), EvalCheck(name='single_question_per_step', passed=False, reason='No interaction data to evaluate.', evidence_snippet=None, confidence=0.2), EvalCheck(name='wait_for_student', passed=False, reason='No interaction data to evaluate.', evidence_snippet=None, confidence=0.2), EvalCheck(name='check_reinforce', passed=False, reason='No interaction data to evaluate.', evidence_snippet=None, confidence=0.2), EvalCheck(name='vary_rhythm', passed=False, reason='No interaction data to evaluate.', evidence_snippet=None, confidence=0.2), EvalCheck(name='conciseness', passed=False, reason='No interaction data to evaluate.', evidence_snippet=None, confidence=0.2)], notes='No agent_instructions and empty regenerated_trajectory; unable to assess criteria.', evaluated_at='2025-09-06T00:00:00Z')]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "await run_eval_batch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aJzUaufjKak"
      },
      "source": [
        "## Initial Evals\n",
        "\n",
        "Once we have a minimally functional system we should process more inputs and get domain\n",
        "experts to help develop ground-truth data. Domain experts doing expert tasks may not\n",
        "have much time to devote to our project, so we want to be efficient and start small,\n",
        "aiming for breadth rather than depth at first.\n",
        "\n",
        "So your evaluation framework should ask:\n",
        "\n",
        "- Given a student-teacher session, how well do we regenerate responses?\n",
        "\n",
        "- Given a regenerated session, how good is the structured judge’s analysis?\n",
        "\n",
        "- Given the session, how accurate is the final escalate decision?\n",
        "\n",
        "### Additional Considerations\n",
        "\n",
        "There's a little more to it than that though, because when you are evaluating a\n",
        "multistep process it's important to know both the end to end performance and the\n",
        "performance of each individual step, *conditioned on the output of the prior step*.\n",
        "\n",
        "What we *want* is to be confident\n",
        "that the auditor is making the correct decision based on the evidence available, even if\n",
        "that evidence is misleading. If we don't pay attention to that case, we can end up\n",
        "training the auditor to ignore its inputs and cause our overall performance to degrade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg2So_MajKak"
      },
      "source": [
        "### Graders\n",
        "\n",
        "The core component of an eval is the\n",
        "[grader](https://platform.openai.com/docs/guides/graders). Our eventual eval is going to\n",
        "use 18 of them, but we only use three kinds, and they're all quite conceptually\n",
        "straightforward.\n",
        "\n",
        "Here are examples of one of our string check graders, one of our text similarity\n",
        "graders, and finally one of our model graders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class EvalInput(BaseModel):\n",
        "    session_id: str\n",
        "    regenerated_trajectory: list  # List of assistant-user turn dicts\n",
        "\n",
        "class EvalOutput(BaseModel):\n",
        "    diagnosis_first: bool\n",
        "    micro_explain: bool\n",
        "    single_question_per_step: bool\n",
        "    pedagogical_fidelity: float\n",
        "    escalate: bool\n",
        "    notes: str\n",
        "\n",
        "rubric = \"\"\"\n",
        "Evaluate the following tutoring session for pedagogical quality.\n",
        "Return a JSON object with fields:\n",
        "- diagnosis_first: true|false\n",
        "- micro_explain: true|false\n",
        "- single_question_per_step: true|false\n",
        "- pedagogical_fidelity: numeric score (0.0–1.0)\n",
        "- escalate: true|false (true if fidelity < 0.5)\n",
        "- notes: brief comment\n",
        "Only output JSON, no additional text.\n",
        "\"\"\"\n",
        "\n",
        "def make_eval_example(session_id, trajectory):\n",
        "    return {\n",
        "        \"session_id\": session_id,\n",
        "        \"regenerated_trajectory\": trajectory\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "from persist_cache import cache\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "client = AsyncOpenAI()\n",
        "\n",
        "rubric = \"\"\"\n",
        "You are an expert educational evaluator. Score this tutoring session on a scale from 1 (poor) to 5 (excellent) based on:\n",
        "1. Clarity of explanation\n",
        "2. Step-by-step guidance\n",
        "3. Engagement with learner\n",
        "4. Correctness of content\n",
        "Respond ONLY with the score number, no extra commentary.\n",
        "\"\"\"\n",
        "\n",
        "@cache\n",
        "async def create_tutoring_eval(name: str):\n",
        "    eval_cfg = await client.evals.create(\n",
        "        name=name,\n",
        "        data_source_config={\n",
        "            \"type\": \"custom\",\n",
        "            \"item_schema\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"session_id\": {\"type\": \"string\"},\n",
        "                    \"regenerated_text\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"session_id\", \"regenerated_text\"],\n",
        "            },\n",
        "            \"include_sample_schema\": False,\n",
        "        },\n",
        "        testing_criteria=[\n",
        "            {\n",
        "                \"type\": \"score_model\",\n",
        "                \"name\": \"Pedagogical Score\",\n",
        "                \"model\": \"gpt-4\",\n",
        "                \"input\": [\n",
        "                    {\"role\": \"system\", \"content\": rubric},\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"{{ item.regenerated_text }}\"\n",
        "                    }\n",
        "                ],\n",
        "                \"range\": [1, 5],\n",
        "                \"pass_threshold\": 3,\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    print(\"Eval created:\", eval_cfg.id)\n",
        "    return eval_cfg\n",
        "\n",
        "async def run_tutoring_eval(name: str, sessions: list[dict]):\n",
        "    eval_cfg = await create_tutoring_eval(name)\n",
        "    run = await client.evals.runs.create(\n",
        "        name=name + \"-run\",\n",
        "        eval_id=eval_cfg.id,\n",
        "        data_source={\n",
        "            \"type\": \"jsonl\",\n",
        "            \"source\": {\"type\": \"file_content\", \"content\": sessions},\n",
        "        },\n",
        "    )\n",
        "    print(\"Eval run started:\", run.report_url)\n",
        "    return run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval run started: https://platform.openai.com/evaluations/eval_68bbb89d63688191ba99f1b5ec74a06b?project_id=proj_JwDXfk2L5W29oc6gMlfsQfbN&run_id=evalrun_68bbbdc0c4388191bbc47fb5dd091933\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RunCreateResponse(id='evalrun_68bbbdc0c4388191bbc47fb5dd091933', created_at=1757134272, data_source=CreateEvalJSONLRunDataSource(source=SourceFileContent(content=[SourceFileContentContent(item={'session_id': 'programming.python.sir_zia.004', 'regenerated_text': 'USER: please teach me python\\nASSISTANT: Awesome—happy to teach you Python. Quick check-in first: what’s your current experience with programming—totally new, a little, or already comfortable?\\nUSER: from zero\\nASSISTANT: Perfect. I’ll be your coach from zero.\\n\\nQuick question to tailor this: What would you like to eventually do with Python — automate small tasks, analyze data, make games, build websites, or something else?\\nUSER: print(\\'Hello, Zia!\\')\\nASSISTANT: Nice — that’s exactly right. You just told Python to print a message.\\n\\nNext step: variables. A variable is like a labeled box where you store a value (like your name) so you can reuse it.\\n\\nYour turn (one small task):\\n- Make a variable called name that stores your name.\\n- Then print the greeting using that variable with an f-string.\\n\\nHint: it looks like this pattern:\\nname = \\'YourName\\'\\nprint(f\\'Hello, {name}!\\')\\n\\nTry it and tell me what happened (or paste what you typed).\\nUSER: color : str = \\'black\\'\\nASSISTANT: Nice! You’ve stored the color correctly. The “: str” is a type hint (optional in Python); plain color = \\'black\\' also works.\\n\\nNow, to display it, what single line would you add to print the value of color?\\nUSER: black\\nASSISTANT: Yes — it would print: black\\n\\nNext step: using a variable inside a sentence.\\nIn Python, f-strings let you “drop” variables into text like this:\\nname = \\'Zia\\'\\nprint(f\"Hello, {name}!\")\\n\\nYour turn (one line): Using your color variable, print a full sentence like:\\nMy favorite color is black.\\nPlease use an f-string. What would you write?\\nUSER: year: int = 2025\\nASSISTANT: Great — that sets year correctly (and nice use of a type hint).\\n\\nNext step: show it on the screen. What single line would you add to display the value of year? Try typing it.\\nUSER: next = year - 10\\nASSISTANT: Great start — that expression does the right math.\\n\\nTwo tiny tweaks:\\n- Avoid the name next (it’s a built-in function in Python). Use something like ten_years_ago or past_year.\\n- Print the result so you can see it.\\n\\nTry:\\nten_years_ago = year - 10\\nprint(ten_years_ago)\\n\\nIf year = 2025, what will this print?\\nUSER: 2035\\nASSISTANT: Correct — 2035.\\n\\nStep 6: Combine text and variables (f-strings)\\nAn f-string lets you insert variable values inside a string using { }.\\n\\nPattern:\\nprint(f\"Some text {variable} more text {other_variable}\")\\n\\nYour turn:\\nUsing your existing variables (name = \\'Zia\\' and color = \\'black\\'), print this in one line:\\nHello, Zia! Your favorite color is black.\\n\\nTry it with an f-string. What do you write?'}, sample=None), SourceFileContentContent(item={'session_id': 'programming.python.intro.001', 'regenerated_text': 'USER: please teach me python\\nASSISTANT: Awesome—happy to help you learn Python. To tailor this to you, what’s your main goal for learning Python right now (e.g., data analysis, web apps, automation, games, or just basics)?\\nUSER: from zero\\nASSISTANT: Awesome. We’ll keep it hands-on and build up step by step.\\n\\nQuick roadmap (so you know where we’re headed):\\n- Lesson 1: Run your first program, print, variables, simple math\\n- Lesson 2: Input and decisions (if/else)\\n- Lesson 3: Loops (for/while)\\n- Lesson 4: Functions\\n- Then: Lists/dicts, small projects\\n\\nFirst, how do you want to run code right now: use an online editor (fast to start) or install Python on your computer?'}, sample=None)], type='file_content'), type='jsonl'), error=None, eval_id='eval_68bbb89d63688191ba99f1b5ec74a06b', metadata={}, model=None, name='TutoringEval-v1-run', object='eval.run', per_model_usage=None, per_testing_criteria_results=None, report_url='https://platform.openai.com/evaluations/eval_68bbb89d63688191ba99f1b5ec74a06b?project_id=proj_JwDXfk2L5W29oc6gMlfsQfbN&run_id=evalrun_68bbbdc0c4388191bbc47fb5dd091933', result_counts=ResultCounts(errored=0, failed=0, passed=0, total=0), status='queued', shared_with_openai=True)"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def flatten_trajectory(trajectory: list[dict]) -> str:\n",
        "    lines = []\n",
        "    for turn in trajectory:\n",
        "        role = turn.get(\"role\", \"unknown\").capitalize()\n",
        "        content = turn.get(\"content\", \"\")\n",
        "        lines.append(f\"{role}: {content}\")\n",
        "    return \"\\n\".join(lines)\n",
        "    \n",
        "def synthesize_text(turns):\n",
        "    parts = []\n",
        "    for t in turns:\n",
        "        role = t[\"role\"]\n",
        "        content = t.get(\"content\", \"\")\n",
        "        parts.append(f\"{role.upper()}: {content}\")\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "def load_sessions(paths):\n",
        "    sessions = []\n",
        "    for path in paths:\n",
        "        data = json.loads(Path(path).read_text())\n",
        "        sessions.append({\n",
        "            \"item\": {  # 👈 required wrapper\n",
        "                \"session_id\": data[\"session_id\"],\n",
        "                \"regenerated_text\": synthesize_text(data[\"regenerated_trajectory\"]),\n",
        "            }\n",
        "        })\n",
        "    return sessions\n",
        "\n",
        "\n",
        "paths = [\n",
        "    \"artifacts/regenerated/programming.python.sir_zia.004.json\",\n",
        "    \"artifacts/regenerated/programming.python.intro.001.json\",\n",
        "]\n",
        "\n",
        "batch = load_sessions(paths)\n",
        "await run_tutoring_eval(\"TutoringEval-v1\", batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyHXlt1vjKak"
      },
      "source": [
        "Each grader evaluates some portion of a predicted output. This might be a very narrow\n",
        "check for a specific field in a structured output, or a more holistic check that\n",
        "judges an output in its entirety. Some graders can work without context, and evaluate an\n",
        "output in isolation (for example, an LLM judge that is evaluating if a paragraph is rude\n",
        "or inappropriate). Others can evaluate based on the input and output, while while the\n",
        "ones we're using here rely on an output and a ground-truth (correct) output to compare\n",
        "against.\n",
        "\n",
        "The most direct way of using Evals provides a prompt and a model, and lets the eval run\n",
        "on an input to generate output itself. Another useful method uses previously logged\n",
        "responses or completions as the source of the outputs. It's not quite as simple, but the\n",
        "most flexible thing we can do is to supply an item containing everything we want it to\n",
        "use—this allows us to have the \"prediction\" function be an arbitrary system rather than\n",
        "restricting it to a single model call. \n",
        "\n",
        "> **Note on Model Selection:**  \n",
        "> Selecting the right model is crucial. While faster, less expensive models are often preferable in production, development workflows benefit from prioritizing the most capable models available. For this guide, we use `o4-mini` for both system tasks and LLM-based grading—while `o3` is more capable, our experience suggests the difference in output quality is modest relative to the substantial increase in cost. In practice, spending $10+/day/engineer on evals is typical, but scaling to $100+/day/engineer may not be sustainable.\n",
        ">\n",
        "> Nonetheless, it's valuable to periodically benchmark with a more advanced model like `o3`. If you observe significant improvements, consider incorporating it for a representative subset of your evaluation data. Discrepancies between models can reveal important edge cases and guide system improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR3gqh5_jKak"
      },
      "source": [
        "Once we have the graders and the data, creating and running our evals is very straightforward:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ls7CaohjKak"
      },
      "source": [
        "## Connecting Evals to Business Metrics\n",
        "\n",
        "Evals show you where you can improve, and help track progress and regressions over time.\n",
        "But the three evals above are just measurements — we need to imbue them with raison\n",
        "d'être.\n",
        "\n",
        "The first thing we need is to add evaluations for the final stage of our receipt\n",
        "processing, so that we can start seeing the results of our audit decisions. The next\n",
        "thing we need, the most important, is a *model of business relevance*.\n",
        "\n",
        "### A Business Model\n",
        "\n",
        "In education, the \"business cost\" isn’t lost dollars per receipt but student outcomes & tutor efficiency.\n",
        "For example:\n",
        "\n",
        "- False Positive (FP) → model says “good learning step” when it’s actually poor → wasted study time.\n",
        "- False Negative (FN) → model says “bad step” when it was useful → discourages learner.\n",
        "- Processing Cost → cost per tutoring trajectory (compute + oversight).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "3zQFrBhTjKal"
      },
      "outputs": [],
      "source": [
        "def calculate_learning_cost(fp_rate: float, fn_rate: float, per_session_cost: float):\n",
        "    study_sessions = 10000  # e.g., 10k tutoring sessions\n",
        "    wasted_time_cost = 5    # $5 value lost per bad step (FP)\n",
        "    missed_opportunity_cost = 20  # $20 lost per missed learning\n",
        "    oversight_cost = per_session_cost\n",
        "\n",
        "    wasted_time = study_sessions * fp_rate * wasted_time_cost\n",
        "    missed_learning = study_sessions * fn_rate * missed_opportunity_cost\n",
        "    processing_cost = study_sessions * oversight_cost\n",
        "\n",
        "    return wasted_time + missed_learning + processing_cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzfd9ecQjKal"
      },
      "source": [
        "\n",
        "### Connecting Back To Evals\n",
        "\n",
        "The point of the above model is it lets us apply meaning to an eval that would\n",
        "otherwise just be a number.\n",
        "\n",
        "Let's Build Multi-Criteria Graders (for Tutoring). Instead of receipt fields, we check pedagogical dimensions:\n",
        "\n",
        "📖 Basic Learning Support\n",
        "\n",
        "- Did the tutor answer the question correctly?\n",
        "- Was the explanation clear?\n",
        "- Did it stay on topic?\n",
        "\n",
        "🧠 Deeper Pedagogy\n",
        "\n",
        "- Did it adapt to student mistakes?\n",
        "- Did it use scaffolding (step-by-step, not dumping answer)?\n",
        "- Was it encouraging & motivational?\n",
        "\n",
        "✨ Reasoning Quality\n",
        "\n",
        "- Did the tutor explain reasoning process?\n",
        "- Score 0–10 on Pedagogical Quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "xaa2sIUUjKal"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Tutoring Evaluation Graders\n",
        "# -----------------------------\n",
        "\n",
        "basic_support_graders = [\n",
        "    {\n",
        "        \"name\": \"Answer Accuracy\",\n",
        "        \"type\": \"score_model\",\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"input\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"\n",
        "Your task is to check whether the tutor gave the correct *final answer* to the student's main question.\n",
        "Score 1 if correct, 0 if incorrect or missing.\n",
        "Trajectory:\n",
        "{{ item.regenerated_trajectory }}\n",
        "\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        \"range\": [0, 1],\n",
        "        \"pass_threshold\": 1,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Clarity of Explanation\",\n",
        "        \"type\": \"score_model\",\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"input\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"\n",
        "Did the tutor explain clearly, step-by-step, in a way a student can follow?\n",
        "0 = totally confusing\n",
        "1 = somewhat clear\n",
        "2 = very clear\n",
        "Trajectory:\n",
        "{{ item.regenerated_trajectory }}\n",
        "\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        \"range\": [0, 2],\n",
        "        \"pass_threshold\": 2,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Staying on Topic\",\n",
        "        \"type\": \"score_model\",\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"input\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"\n",
        "Check if the tutor stayed focused on the student’s question instead of going off-topic.\n",
        "Score 1 if yes, 0 if no.\n",
        "Trajectory:\n",
        "{{ item.regenerated_trajectory }}\n",
        "\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        \"range\": [0, 1],\n",
        "        \"pass_threshold\": 1,\n",
        "    },\n",
        "]\n",
        "\n",
        "deeper_pedagogy_graders = [\n",
        "    {\n",
        "        \"name\": \"Adaptivity to Mistakes\",\n",
        "        \"type\": \"score_model\",\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"input\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"\n",
        "Evaluate how well the tutor adapted to the student’s mistakes or misunderstandings.\n",
        "Score:\n",
        "0 = did not notice/respond\n",
        "1 = noticed but weak adaptation\n",
        "2 = strong adaptation with correction\n",
        "Trajectory:\n",
        "{{ item.regenerated_trajectory }}\n",
        "\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        \"range\": [0, 2],\n",
        "        \"pass_threshold\": 1,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Scaffolding Support\",\n",
        "        \"type\": \"score_model\",\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"input\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"\n",
        "Did the tutor scaffold the learning (guiding step by step, rather than dumping answers)?\n",
        "0 = no scaffolding\n",
        "1 = partial scaffolding\n",
        "2 = strong scaffolding\n",
        "Trajectory:\n",
        "{{ item.regenerated_trajectory }}\n",
        "\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        \"range\": [0, 2],\n",
        "        \"pass_threshold\": 1,\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Encouragement & Motivation\",\n",
        "        \"type\": \"score_model\",\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"input\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"\n",
        "Evaluate the tutor’s tone:\n",
        "0 = discouraging or neutral\n",
        "1 = mildly supportive\n",
        "2 = encouraging and motivational\n",
        "Trajectory:\n",
        "{{ item.regenerated_trajectory }}\n",
        "\"\"\"\n",
        "            }\n",
        "        ],\n",
        "        \"range\": [0, 2],\n",
        "        \"pass_threshold\": 1,\n",
        "    },\n",
        "]\n",
        "\n",
        "reasoning_eval_prompt = \"\"\"\n",
        "Your task is to evaluate the *reasoning quality* of the tutor.\n",
        "\n",
        "Check:\n",
        "1. Did the tutor logically explain the reasoning steps?\n",
        "2. Were explanations accurate and pedagogically sound?\n",
        "3. Was the reasoning concise (not rambling)?\n",
        "4. Did the tutor reach a correct final conclusion?\n",
        "\n",
        "Grade 0–10:\n",
        "- 0 = incoherent or wrong reasoning\n",
        "- 5 = partially correct, missing some logic\n",
        "- 10 = clear, step-by-step, correct reasoning\n",
        "Trajectory:\n",
        "{{ item.regenerated_trajectory }}\n",
        "\"\"\"\n",
        "\n",
        "reasoning_graders = [\n",
        "    {\n",
        "        \"name\": \"Pedagogical Reasoning Quality\",\n",
        "        \"type\": \"score_model\",\n",
        "        \"model\": \"gpt-4\",\n",
        "        \"input\": [{\"role\": \"system\", \"content\": reasoning_eval_prompt}],\n",
        "        \"range\": [0, 10],\n",
        "        \"pass_threshold\": 8,\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Create Full Tutoring Eval\n",
        "# -----------------------------\n",
        "\n",
        "full_eval = await client.evals.create(\n",
        "    name=\"Full-StudyMode-Tutoring-Eval\",\n",
        "    data_source_config={\n",
        "        \"type\": \"custom\",\n",
        "        \"item_schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"session_id\": {\"type\": \"string\"},\n",
        "                \"regenerated_trajectory\": {\"type\": \"array\"},\n",
        "            },\n",
        "            \"required\": [\"session_id\", \"regenerated_trajectory\"],\n",
        "        },\n",
        "    },\n",
        "    testing_criteria=(\n",
        "        basic_support_graders\n",
        "        + deeper_pedagogy_graders\n",
        "        + reasoning_graders\n",
        "    ),\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def flatten_trajectory(trajectory: list[dict]) -> str:\n",
        "    lines = []\n",
        "    for turn in trajectory:\n",
        "        role = turn.get(\"role\", \"unknown\").capitalize()\n",
        "        content = turn.get(\"content\", \"\")\n",
        "        lines.append(f\"{role}: {content}\")\n",
        "    return \"\\n\".join(lines)\n",
        "    \n",
        "def prepare_eval_batch(paths: list[str]):\n",
        "    batch = []\n",
        "    for path_str in paths:\n",
        "        path = Path(path_str)\n",
        "        if not path.exists():\n",
        "            continue\n",
        "\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            session = json.load(f)\n",
        "\n",
        "        if \"session_id\" not in session or \"regenerated_trajectory\" not in session:\n",
        "            continue\n",
        "\n",
        "        flat_traj = flatten_trajectory(session[\"regenerated_trajectory\"])\n",
        "\n",
        "        batch.append({\n",
        "            \"item\": {\n",
        "                \"session_id\": session[\"session_id\"],\n",
        "                \"regenerated_trajectory\": flat_traj  # <--- string now\n",
        "            }\n",
        "        })\n",
        "\n",
        "    return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Report URL: https://platform.openai.com/evaluations/eval_68bbbb15464481919b688139a74e9e15?project_id=proj_JwDXfk2L5W29oc6gMlfsQfbN&run_id=evalrun_68bbbe2163e88191b27318aef97f781d\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def load_sessions(paths: list[str]):\n",
        "    batch = []\n",
        "    for p in paths:\n",
        "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "            session = json.load(f)\n",
        "\n",
        "        batch.append({\n",
        "            \"item\": {   # must wrap in \"item\"\n",
        "                \"session_id\": session[\"session_id\"],\n",
        "                \"regenerated_trajectory\": session[\"regenerated_trajectory\"],  # keep as array\n",
        "            }\n",
        "        })\n",
        "    return batch\n",
        "\n",
        "\n",
        "paths = [\n",
        "    \"artifacts/regenerated/programming.python.sir_zia.004.json\",\n",
        "    \"artifacts/regenerated/programming.python.intro.001.json\",\n",
        "]\n",
        "\n",
        "batch = load_sessions(paths)\n",
        "\n",
        "eval_run = await client.evals.runs.create(\n",
        "    name=\"study-mode-full-eval-run\",\n",
        "    eval_id=full_eval.id,\n",
        "    data_source={\n",
        "        \"type\": \"jsonl\",\n",
        "        \"source\": {\"type\": \"file_content\", \"content\": batch},\n",
        "    },\n",
        ")\n",
        "\n",
        "print(\"Eval Report URL:\", eval_run.report_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm3fWRHtjKal"
      },
      "source": [
        "## Spin Up the Flywheel\n",
        "\n",
        "Having our business model means we have a map of what's worth doing and what isn't. Our\n",
        "initial evals are a road sign that lets us know we're moving in the right direction; but\n",
        "eventually we'll need more signage. At this point in the process we usually have a lot\n",
        "of different things we can work on, with a few linked cycles where improvement on one\n",
        "will open up more room for improvement on a different cycle.\n",
        "\n",
        "![Development Flywheel](https://github.com/openai/openai-cookbook/blob/main/images/partner_development_flywheel.png?raw=1)\n",
        "\n",
        "1. Our evals show us where we can improve, and we can immediately use them to guide us\n",
        "   in model selection, prompt engineering, tool use, and fine-tuning strategies.\n",
        "2. We're not done once system performs well according to our evals. That's when it's\n",
        "   time to *improve our evals*. We will process more data, give it to our domain experts\n",
        "   to review, and feed the corrections into building better, more comprehensive evals.\n",
        "\n",
        "This cycle can go on for a while. We can speed it along by identifying the efficient\n",
        "frontier of \"interesting\" data to examine. There are a few techniques for this, but an\n",
        "easy one is re-running models on inputs to prioritize labeling inputs that don't\n",
        "get consistent answers. This works especially well when using different underlying\n",
        "models, and often even benefits from using less-intelligent models (if a dumb model\n",
        "agrees with a smart model then it's probably not a hard problem).\n",
        "\n",
        "Once it seems like we've hit a point of dimishing returns on performance, we can keep\n",
        "using the same techniques to optimize model cost; if we have a system that performs\n",
        "quite well, then fine-tuning or some form of model distillation will probably allow us\n",
        "to get similar performance from smaller, cheaper, faster models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-7usWUNjKal"
      },
      "source": [
        "## System Improvements\n",
        "\n",
        "With our evals in place and an understanding of how they connect to our business metrics,\n",
        "we're finally ready to turn our attention to improving the output of our system.\n",
        "\n",
        "Let's modify the prompt and re-run our evals to see how we do. We'll provide more\n",
        "guidance in the form of a specific example in the instructions about engine oil\n",
        "(different from a snow broom, but requires the same reasoning), and we'll include three\n",
        "examples pulled from our training set (`data/train`) as few-shot guidance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7K4tyrAjKay"
      },
      "source": [
        "When we ran the eval again, we actually still got two audit decisions wrong. Digging into\n",
        "the examples we made a mistake on, it turns out that we completely fixed the issues we\n",
        "identified, but our examples improved the reasoning step and caused two other issues to\n",
        "surface. \n",
        "\n",
        "This is great, and we'll continue iterating on issues as we uncover them. This is the\n",
        "cycle of improvement!\n",
        "\n",
        "### Model Choice\n",
        "\n",
        "When beginning a project, we usually start with one of the most capable models available, such as `o4-mini`, to establish a performance baseline. Once we’re confident in the model’s ability to solve the task, the next step is to explore smaller, faster, or more cost-effective alternatives.\n",
        "\n",
        "Optimizing for inference cost and latency is essential, especially for production or customer-facing systems, where these factors can significantly impact overall expenses and user experience. For instance, switching from `o4-mini` to `gpt-4.1-mini` could reduce inference costs by nearly two-thirds—an example where thoughtful model selection leads to meaningful savings.\n",
        "\n",
        "In the next section, we’ll rerun our evaluations using `gpt-4.1-mini` for both extraction and audit steps to see how well a more efficient model performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhnuCuadjKay"
      },
      "source": [
        "\n",
        "### Further improvements\n",
        "\n",
        "This cookbook focuses on the philosophy and practicalities of evals, not the full range of model improvement techniques. For boosting or maintaining model performance (especially when moving to smaller, faster, or cheaper models), consider these steps in order—start from the top, and only proceed down if needed. For example, always optimize your prompt before resorting to fine-tuning; fine-tuning on a weak prompt can lock in bad performance even if you improve the prompt later.\n",
        "\n",
        "![Model Improvement Waterfall](https://github.com/openai/openai-cookbook/blob/main/images/partner_model_improvement_waterfall.png?raw=1)\n",
        "\n",
        "1. **Model selection:** try smarter models, or increase their reasoning budget.\n",
        "2. **Prompt tuning:** clarify instructions and provide very explicit rules.\n",
        "3. **Examples and context:** add few- or many-shot examples, or more context for the\n",
        "   problem. RAG fits in here, and may be used to dynamically select similar examples.\n",
        "4. **Tools use:** provide tools to solve specific problems, including access to external\n",
        "   APIs, the ability to query databases, or otherwise enable the model to have its own\n",
        "   questions answered.\n",
        "5. **Accessory models:** add models to perform limited sub-tasks, to supervise and provide\n",
        "   guardrails, or use a mixture of experts and aggregate solutions from multiple\n",
        "   sub-models.\n",
        "6. **Fine-tuning:** use labeled training data for supervised fine tuning, eval\n",
        "   graders for reinforcement fine tuning, or different outputs for direct preference\n",
        "   optimization.\n",
        "\n",
        "The above options are all tools to maximize performance. Once you're trying to optimize\n",
        "for a price:performance ratio, you'll usually have already done all of the above and\n",
        "likely don't need to repeat most steps, but you can still fine-tune smaller models or\n",
        "use your best model to train a smaller model (model distillation).\n",
        "\n",
        "> One really excellent thing about OpenAI Evals is that you can use the same graders for\n",
        "> [Reinforcement Fine-Tuning](https://cookbook.openai.com/examples/reinforcement_fine_tuning)\n",
        "> to produce better model performance in an extremely sample-efficient manner. One note\n",
        "> of caution is to make sure that you use separate training data and don't leak your\n",
        "> eval datasets during RFT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC-9UGEnjKaz"
      },
      "source": [
        "## Deploying and Post-Development\n",
        "Building and deploying an LLM application is just the beginning—the real value comes from ongoing improvement. Once your system is live, prioritize continuous monitoring: log traces, track outputs, and proactively sample real user interactions for human review using smart sampling techniques.\n",
        "\n",
        "Production data is your most authentic source for evolving your evaluation and training datasets. Regularly collect and curate fresh samples from actual use cases to identify gaps, edge cases, and new opportunities for enhancement.\n",
        "\n",
        "In practice, leverage this data for rapid iteration. Automate periodic fine-tuning pipelines that retrain your models on recent, high-quality samples and automatically deploy new versions when they outperform existing ones in your evals. Capture user corrections and feedback, then systematically feed these insights back into your prompts or retraining process—especially when they highlight persistent issues.\n",
        "\n",
        "By embedding these feedback loops into your post-development workflow, you ensure your LLM applications continuously adapt, stay robust, and remain closely aligned with user needs as they evolve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O8gek5qjKaz"
      },
      "source": [
        "## CORE Resources\n",
        "\n",
        "https://cookbook.openai.com/examples/partners/eval_driven_system_design/receipt_inspection\n",
        "\n",
        "https://chatgpt.com/share/68b7f90f-146c-8002-9a95-75844a9355fd\n",
        "\n",
        "https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221l0Pg8Fv6Pj0JVR80ZJCLtNuYWWViSO1R%22%5D,%22action%22:%22open%22,%22userId%22:%22106656036806020573706%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing\n",
        "\n",
        "https://chatgpt.com/share/68bb49c5-ca1c-8002-b8c6-61d589f0e86b\n",
        "\n",
        "## RESEARCH PAPERS\n",
        "https://arxiv.org/html/2507.21504v1\n",
        "\n",
        "https://arxiv.org/html/2411.13768v2\n",
        "\n",
        "https://arxiv.org/html/2502.06329v1\n",
        "\n",
        "https://arxiv.org/pdf/2504.08942\n",
        "\n",
        "## YT VIDEOS\n",
        "\n",
        "https://www.youtube.com/watch?v=a4BV0gGmXgA\n",
        "\n",
        "https://www.youtube.com/watch?v=RAEyhC0P2Ic\n",
        "\n",
        "https://www.youtube.com/watch?v=4QXtObc61Lw\n",
        "\n",
        "## CHAT SESSIONS\n",
        "\n",
        "https://chatgpt.com/share/68bb48be-75a0-8002-9ad2-c69985e39155\n",
        "\n",
        "https://chatgpt.com/share/68b4024e-7f88-8001-9a65-f06680de77f5\n",
        "\n",
        "https://chatgpt.com/share/68b3ff88-0f74-8001-b97f-166ff6821b30\n",
        "\n",
        "https://chatgpt.com/share/68b3ff88-0f74-8001-b97f-166ff6821b30\n",
        "\n",
        "## Extra Resources\n",
        "\n",
        "https://platform.openai.com/chat/edit?models=gpt-5&optimize=true\n",
        "\n",
        "https://cookbook.openai.com/examples/realtime_prompting_guide\n",
        "\n",
        "\n",
        "https://webinar.openai.com/on-demand/d1a99ac5-8de8-43c5-b209-21903d76b5b2\n",
        "\n",
        "\n",
        "https://github.com/openai/build-hours/tree/main\n",
        "\n",
        "\n",
        "https://cdn.openai.com/business-guides-and-resources/identifying-and-scaling-ai-use-cases.pdf\n",
        "\n",
        "https://webinar.openai.com/buildhours/\n",
        "\n",
        "https://cdn.openai.com/business-guides-and-resources/ai-in-the-enterprise.pdf\n",
        "\n",
        "https://github.blog/ai-and-ml/generative-ai/spec-driven-development-with-ai-get-started-with-a-new-open-source-toolkit/\n",
        "\n",
        "https://cookbook.openai.com/examples/gpt-5/gpt-5_new_params_and_tools#3-contextfree-grammar-cfg\n",
        "\n",
        "https://docs.ragas.io/en/stable/concepts/test_data_generation/rag/\n",
        "\n",
        "https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide\n",
        "\n",
        "https://github.com/langchain-ai/agentevals"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
